{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Imports\n",
    "# ================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import nltk\n",
    "import yaml\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr, spearmanr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Oana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Setup\n",
    "# ================================\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Preprocessing Functions\n",
    "# ================================\n",
    "\n",
    "def preprocess_sentence_spacy(sentence, tokenize=True):\n",
    "    \"\"\"\n",
    "    Preprocesează o propoziție folosind spaCy.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Propoziția de preprocesat.\n",
    "        tokenize (bool): Dacă este True, se tokenizează propoziția.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tokeni.\n",
    "    \"\"\"\n",
    "    if tokenize:\n",
    "        doc = nlp(sentence.lower())\n",
    "        return [token.text for token in doc]\n",
    "    else:\n",
    "        return sentence.lower().split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "This file contains the definition of encoders used in https://arxiv.org/pdf/1705.02364.pdf\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class InferSent(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(InferSent, self).__init__()\n",
    "        self.bsize = config['bsize']\n",
    "        self.word_emb_dim = config['word_emb_dim']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.pool_type = config['pool_type']\n",
    "        self.dpout_model = config['dpout_model']\n",
    "        self.version = 1 if 'version' not in config else config['version']\n",
    "\n",
    "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n",
    "                                bidirectional=True, dropout=self.dpout_model)\n",
    "\n",
    "        assert self.version in [1, 2]\n",
    "        if self.version == 1:\n",
    "            self.bos = '<s>'\n",
    "            self.eos = '</s>'\n",
    "            self.max_pad = True\n",
    "            self.moses_tok = False\n",
    "        elif self.version == 2:\n",
    "            self.bos = '<p>'\n",
    "            self.eos = '</p>'\n",
    "            self.max_pad = False\n",
    "            self.moses_tok = True\n",
    "\n",
    "    def is_cuda(self):\n",
    "        # either all weights are on cpu or they are on gpu\n",
    "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
    "\n",
    "    def forward(self, sent_tuple):\n",
    "        # sent_len: [max_len, ..., min_len] (bsize)\n",
    "        # sent: (seqlen x bsize x worddim)\n",
    "        sent, sent_len = sent_tuple\n",
    "\n",
    "        # Sort by length (keep idx)\n",
    "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
    "        sent_len_sorted = sent_len_sorted.copy()\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "\n",
    "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n",
    "            else torch.from_numpy(idx_sort)\n",
    "        sent = sent.index_select(1, idx_sort)\n",
    "\n",
    "        # Handling padding in Recurrent Networks\n",
    "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
    "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
    "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
    "\n",
    "        # Un-sort by length\n",
    "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
    "            else torch.from_numpy(idx_unsort)\n",
    "        sent_output = sent_output.index_select(1, idx_unsort)\n",
    "\n",
    "        # Pooling\n",
    "        if self.pool_type == \"mean\":\n",
    "            sent_len = torch.FloatTensor(sent_len.copy()).unsqueeze(1).cuda()\n",
    "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
    "            emb = emb / sent_len.expand_as(emb)\n",
    "        elif self.pool_type == \"max\":\n",
    "            if not self.max_pad:\n",
    "                sent_output[sent_output == 0] = -1e9\n",
    "            emb = torch.max(sent_output, 0)[0]\n",
    "            if emb.ndimension() == 3:\n",
    "                emb = emb.squeeze(0)\n",
    "                assert emb.ndimension() == 2\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def set_w2v_path(self, w2v_path):\n",
    "        self.w2v_path = w2v_path\n",
    "\n",
    "    def get_word_dict(self, sentences, tokenize=True):\n",
    "        # create vocab of words\n",
    "        word_dict = {}\n",
    "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
    "        for sent in sentences:\n",
    "            for word in sent:\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = ''\n",
    "        word_dict[self.bos] = ''\n",
    "        word_dict[self.eos] = ''\n",
    "        return word_dict\n",
    "\n",
    "    def get_w2v(self, word_dict):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with w2v vectors\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if word in word_dict:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
    "        return word_vec\n",
    "\n",
    "    def get_w2v_k(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with k first w2v vectors\n",
    "        k = 0\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if k <= K:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "                    k += 1\n",
    "                if k > K:\n",
    "                    if word in [self.bos, self.eos]:\n",
    "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\n",
    "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
    "                    break\n",
    "        return word_vec\n",
    "\n",
    "    def build_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "        self.word_vec = self.get_w2v(word_dict)\n",
    "        print('Vocab size : %s' % (len(self.word_vec)))\n",
    "\n",
    "    # build w2v vocab with k most frequent words\n",
    "    def build_vocab_k_words(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        self.word_vec = self.get_w2v_k(K)\n",
    "        print('Vocab size : %s' % (K))\n",
    "\n",
    "    def update_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
    "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "\n",
    "        # keep only new words\n",
    "        for word in self.word_vec:\n",
    "            if word in word_dict:\n",
    "                del word_dict[word]\n",
    "\n",
    "        # udpate vocabulary\n",
    "        if word_dict:\n",
    "            new_word_vec = self.get_w2v(word_dict)\n",
    "            self.word_vec.update(new_word_vec)\n",
    "        else:\n",
    "            new_word_vec = []\n",
    "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
    "\n",
    "    def get_batch(self, batch):\n",
    "        # sent in batch in decreasing order of lengths\n",
    "        # batch: (bsize, max_len, word_dim)\n",
    "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(len(batch[i])):\n",
    "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
    "\n",
    "        return torch.FloatTensor(embed)\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        return [token.text for token in nlp(s)]\n",
    "\n",
    "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
    "        if not tokenize:\n",
    "            # sentences este o listă de liste de tokeni\n",
    "            sentences = [[self.bos] + s + [self.eos] for s in sentences]\n",
    "        else:\n",
    "            # sentences este o listă de șiruri de caractere\n",
    "            sentences = [[self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
    "        n_w = np.sum([len(x) for x in sentences])\n",
    "\n",
    "        # filters words without w2v vectors\n",
    "        for i in range(len(sentences)):\n",
    "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
    "            if not s_f:\n",
    "                import warnings\n",
    "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
    "                           Replacing by \"%s\"..' % (sentences[i], i, self.eos))\n",
    "                sf = [self.eos]\n",
    "            sentences[i] = s_f\n",
    "\n",
    "        lengths = np.array([len(s) for s in sentences])\n",
    "        n_wk = np.sum(lengths)\n",
    "        if verbose:\n",
    "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
    "                    n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
    "\n",
    "        # sort by decreasing length\n",
    "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
    "        sentences = [sentences[i] for i in idx_sort]\n",
    "\n",
    "        return sentences, lengths, idx_sort\n",
    "\n",
    "\n",
    "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
    "        tic = time.time()\n",
    "\n",
    "        # Pregătirea eșantioanelor\n",
    "        sentences, lengths, idx_sort = self.prepare_samples(\n",
    "            sentences, bsize, tokenize, verbose)\n",
    "\n",
    "        embeddings = []\n",
    "        for stidx in range(0, len(sentences), bsize):\n",
    "            batch_sentences = sentences[stidx:stidx + bsize]\n",
    "            batch_lengths = lengths[stidx:stidx + bsize]\n",
    "\n",
    "            batch = self.get_batch(batch_sentences)\n",
    "            if self.is_cuda():\n",
    "                batch = batch.cuda()\n",
    "            with torch.no_grad():\n",
    "                batch_embeddings = self.forward((batch, batch_lengths)).data.cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "        embeddings = np.vstack(embeddings)\n",
    "\n",
    "        # Un-sort\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        embeddings = embeddings[idx_unsort]\n",
    "\n",
    "        if verbose:\n",
    "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
    "                    len(embeddings)/(time.time()-tic),\n",
    "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
    "        return embeddings\n",
    "\n",
    "    def visualize(self, sent, tokenize=True):\n",
    "\n",
    "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
    "        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n",
    "\n",
    "        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n",
    "            import warnings\n",
    "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n",
    "                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
    "        batch = self.get_batch(sent)\n",
    "\n",
    "        if self.is_cuda():\n",
    "            batch = batch.cuda()\n",
    "        output = self.enc_lstm(batch)[0]\n",
    "        output, idxs = torch.max(output, 0)\n",
    "        # output, idxs = output.squeeze(), idxs.squeeze()\n",
    "        idxs = idxs.data.cpu().numpy()\n",
    "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
    "\n",
    "        # visualize model\n",
    "        import matplotlib.pyplot as plt\n",
    "        x = range(len(sent[0]))\n",
    "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
    "        plt.xticks(x, sent[0], rotation=45)\n",
    "        plt.bar(x, y)\n",
    "        plt.ylabel('%')\n",
    "        plt.title('Visualisation of words importance')\n",
    "        plt.show()\n",
    "\n",
    "        return output, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization successful using spaCy!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "# Încarcă modelul de limbă în spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_sentence_spacy(sentence, tokenize=True):\n",
    "    if tokenize:\n",
    "        doc = nlp(sentence.lower())\n",
    "        return [token.text for token in doc]\n",
    "    else:\n",
    "        return sentence.lower().split()\n",
    "\n",
    "# Încarcă setul de date\n",
    "data = pd.read_csv(r'C:\\facultate an 3\\projects-simquery\\data\\sts_train.csv', delimiter='\\t')\n",
    "\n",
    "# Filtrează datele pentru a elimina rândurile cu valori lipsă\n",
    "data = data.dropna(subset=['sent_1', 'sent_2', 'sim'])\n",
    "\n",
    "# Extrage propozițiile și scorurile\n",
    "sentences_1 = data['sent_1'].tolist()\n",
    "sentences_2 = data['sent_2'].tolist()\n",
    "similarities = data['sim'].tolist()\n",
    "\n",
    "# Tokenizează propozițiile folosind spaCy\n",
    "sentences_1 = [preprocess_sentence_spacy(s) for s in sentences_1]\n",
    "sentences_2 = [preprocess_sentence_spacy(s) for s in sentences_2]\n",
    "\n",
    "print(\"Tokenization successful using spaCy!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Oana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11220(/12518) words with w2v vectors\n",
      "Vocab size : 11220\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Configurarea parametrilor modelului\n",
    "params_model = {\n",
    "    'bsize': 64,\n",
    "    'word_emb_dim': 300,\n",
    "    'enc_lstm_dim': 2048,\n",
    "    'pool_type': 'max',\n",
    "    'dpout_model': 0.0,\n",
    "    'version': 1,\n",
    "}\n",
    "\n",
    "model = InferSent(params_model)\n",
    "\n",
    "model.set_w2v_path(r'C:\\facultate an 3\\projects-simquery\\GloVe\\glove.840B.300d.txt')\n",
    "\n",
    "# Construirea vocabularului pe baza propozițiilor din setul de date\n",
    "sentences = [' '.join(s) for s in sentences_1 + sentences_2]  # combină tokenii înapoi în propoziții\n",
    "model.build_vocab(sentences, tokenize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Continuă să combini tokenii înapoi în propoziții\n",
    "sentences_1_str = [' '.join(s) for s in sentences_1]\n",
    "sentences_2_str = [' '.join(s) for s in sentences_2]\n",
    "\n",
    "embeddings_1 = model.encode(sentences_1_str, tokenize=True)\n",
    "embeddings_2 = model.encode(sentences_2_str, tokenize=True)\n",
    "\n",
    "print(\"Embeddings generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convertește embedding-urile și scorurile în numpy arrays\n",
    "X1 = np.array(embeddings_1)\n",
    "X2 = np.array(embeddings_2)\n",
    "y = np.array(similarities).astype(float)\n",
    "\n",
    "# Normalizarea scorurilor\n",
    "y = y / 5.0  # Dacă scorurile sunt între 0 și 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(\n",
    "    X1, X2, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimilarityModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size * 4, 1024),  # Increased input size for more features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Added dropout for regularization\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1)  # Output layer\n",
    "        )\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # Compute interaction features\n",
    "        diff = torch.abs(x1 - x2)\n",
    "        prod = x1 * x2\n",
    "        features = torch.cat([x1, x2, diff, prod], dim=1)\n",
    "        output = self.fc(features)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class STSDataset(Dataset):\n",
    "    def __init__(self, embeddings1, embeddings2, similarities):\n",
    "        self.embeddings1 = embeddings1\n",
    "        self.embeddings2 = embeddings2\n",
    "        self.similarities = similarities\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.similarities)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.embeddings1[idx]\n",
    "        x2 = self.embeddings2[idx]\n",
    "        y = self.similarities[idx]\n",
    "        return x1, x2, y\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = STSDataset(X1_train, X2_train, y_train)\n",
    "val_dataset = STSDataset(X1_val, X2_val, y_val)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bfi2ifz8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">InferSent</strong> at: <a href='https://wandb.ai/oana9700-babes-bolyai-university/inferSent-project/runs/bfi2ifz8' target=\"_blank\">https://wandb.ai/oana9700-babes-bolyai-university/inferSent-project/runs/bfi2ifz8</a><br/> View project at: <a href='https://wandb.ai/oana9700-babes-bolyai-university/inferSent-project' target=\"_blank\">https://wandb.ai/oana9700-babes-bolyai-university/inferSent-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241207_152133-bfi2ifz8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bfi2ifz8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\facultate an 3\\projects-simquery\\app\\models\\model_1\\wandb\\run-20241207_160859-j16y4hmg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oana9700-babes-bolyai-university/inferSent-project/runs/j16y4hmg' target=\"_blank\">InferSent</a></strong> to <a href='https://wandb.ai/oana9700-babes-bolyai-university/inferSent-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oana9700-babes-bolyai-university/inferSent-project' target=\"_blank\">https://wandb.ai/oana9700-babes-bolyai-university/inferSent-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oana9700-babes-bolyai-university/inferSent-project/runs/j16y4hmg' target=\"_blank\">https://wandb.ai/oana9700-babes-bolyai-university/inferSent-project/runs/j16y4hmg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Oana\\AppData\\Local\\Temp\\ipykernel_24816\\1009365268.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x1_batch = torch.tensor(x1_batch).to(device).float()\n",
      "C:\\Users\\Oana\\AppData\\Local\\Temp\\ipykernel_24816\\1009365268.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x2_batch = torch.tensor(x2_batch).to(device).float()\n",
      "C:\\Users\\Oana\\AppData\\Local\\Temp\\ipykernel_24816\\1009365268.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch).to(device).float()\n",
      "C:\\Users\\Oana\\AppData\\Local\\Temp\\ipykernel_24816\\1009365268.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x1_batch = torch.tensor(x1_batch).to(device).float()\n",
      "C:\\Users\\Oana\\AppData\\Local\\Temp\\ipykernel_24816\\1009365268.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x2_batch = torch.tensor(x2_batch).to(device).float()\n",
      "C:\\Users\\Oana\\AppData\\Local\\Temp\\ipykernel_24816\\1009365268.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_batch = torch.tensor(y_batch).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Training Loss: 0.6775, Training Accuracy: 0.6387, Validation Loss: 0.6514, Validation Accuracy: 0.7493\n",
      "Validation loss decreased. Model saved to best_model_epoch_1.pth\n",
      "Epoch 2/40, Training Loss: 0.6330, Training Accuracy: 0.7423, Validation Loss: 0.6078, Validation Accuracy: 0.7711\n",
      "Validation loss decreased. Model saved to best_model_epoch_2.pth\n",
      "Epoch 3/40, Training Loss: 0.6121, Training Accuracy: 0.7516, Validation Loss: 0.6079, Validation Accuracy: 0.7798\n",
      "No improvement in validation loss for 1 epochs.\n",
      "Epoch 4/40, Training Loss: 0.6013, Training Accuracy: 0.7651, Validation Loss: 0.5954, Validation Accuracy: 0.7859\n",
      "Validation loss decreased. Model saved to best_model_epoch_4.pth\n",
      "Epoch 5/40, Training Loss: 0.5946, Training Accuracy: 0.7725, Validation Loss: 0.5943, Validation Accuracy: 0.7807\n",
      "Validation loss decreased. Model saved to best_model_epoch_5.pth\n",
      "Epoch 6/40, Training Loss: 0.5888, Training Accuracy: 0.7869, Validation Loss: 0.5854, Validation Accuracy: 0.7868\n",
      "Validation loss decreased. Model saved to best_model_epoch_6.pth\n",
      "Epoch 7/40, Training Loss: 0.5846, Training Accuracy: 0.7862, Validation Loss: 0.5848, Validation Accuracy: 0.7894\n",
      "Validation loss decreased. Model saved to best_model_epoch_7.pth\n",
      "Epoch 8/40, Training Loss: 0.5809, Training Accuracy: 0.7908, Validation Loss: 0.5829, Validation Accuracy: 0.7842\n",
      "Validation loss decreased. Model saved to best_model_epoch_8.pth\n",
      "Epoch 9/40, Training Loss: 0.5766, Training Accuracy: 0.8006, Validation Loss: 0.5818, Validation Accuracy: 0.7859\n",
      "Validation loss decreased. Model saved to best_model_epoch_9.pth\n",
      "Epoch 10/40, Training Loss: 0.5749, Training Accuracy: 0.8024, Validation Loss: 0.5778, Validation Accuracy: 0.7929\n",
      "Validation loss decreased. Model saved to best_model_epoch_10.pth\n",
      "Epoch 11/40, Training Loss: 0.5721, Training Accuracy: 0.8071, Validation Loss: 0.5811, Validation Accuracy: 0.7868\n",
      "No improvement in validation loss for 1 epochs.\n",
      "Epoch 12/40, Training Loss: 0.5697, Training Accuracy: 0.8154, Validation Loss: 0.5757, Validation Accuracy: 0.7955\n",
      "Validation loss decreased. Model saved to best_model_epoch_12.pth\n",
      "Epoch 13/40, Training Loss: 0.5669, Training Accuracy: 0.8115, Validation Loss: 0.5768, Validation Accuracy: 0.7937\n",
      "No improvement in validation loss for 1 epochs.\n",
      "Epoch 14/40, Training Loss: 0.5647, Training Accuracy: 0.8219, Validation Loss: 0.5732, Validation Accuracy: 0.8068\n",
      "Validation loss decreased. Model saved to best_model_epoch_14.pth\n",
      "Epoch 15/40, Training Loss: 0.5655, Training Accuracy: 0.8148, Validation Loss: 0.5754, Validation Accuracy: 0.7972\n",
      "No improvement in validation loss for 1 epochs.\n",
      "Epoch 16/40, Training Loss: 0.5608, Training Accuracy: 0.8304, Validation Loss: 0.5733, Validation Accuracy: 0.8068\n",
      "No improvement in validation loss for 2 epochs.\n",
      "Epoch 17/40, Training Loss: 0.5589, Training Accuracy: 0.8322, Validation Loss: 0.5718, Validation Accuracy: 0.8077\n",
      "Validation loss decreased. Model saved to best_model_epoch_17.pth\n",
      "Epoch 18/40, Training Loss: 0.5556, Training Accuracy: 0.8433, Validation Loss: 0.5718, Validation Accuracy: 0.8103\n",
      "No improvement in validation loss for 1 epochs.\n",
      "Epoch 19/40, Training Loss: 0.5550, Training Accuracy: 0.8424, Validation Loss: 0.5720, Validation Accuracy: 0.8033\n",
      "No improvement in validation loss for 2 epochs.\n",
      "Epoch 20/40, Training Loss: 0.5543, Training Accuracy: 0.8376, Validation Loss: 0.5707, Validation Accuracy: 0.8094\n",
      "Validation loss decreased. Model saved to best_model_epoch_20.pth\n",
      "Epoch 21/40, Training Loss: 0.5537, Training Accuracy: 0.8472, Validation Loss: 0.5838, Validation Accuracy: 0.7929\n",
      "No improvement in validation loss for 1 epochs.\n",
      "Epoch 22/40, Training Loss: 0.5496, Training Accuracy: 0.8546, Validation Loss: 0.5708, Validation Accuracy: 0.8138\n",
      "No improvement in validation loss for 2 epochs.\n",
      "Epoch 23/40, Training Loss: 0.5467, Training Accuracy: 0.8618, Validation Loss: 0.5736, Validation Accuracy: 0.8007\n",
      "No improvement in validation loss for 3 epochs.\n",
      "Epoch 24/40, Training Loss: 0.5440, Training Accuracy: 0.8600, Validation Loss: 0.5706, Validation Accuracy: 0.8138\n",
      "Validation loss decreased. Model saved to best_model_epoch_24.pth\n",
      "Epoch 25/40, Training Loss: 0.5412, Training Accuracy: 0.8679, Validation Loss: 0.5707, Validation Accuracy: 0.8103\n",
      "No improvement in validation loss for 1 epochs.\n",
      "Epoch 26/40, Training Loss: 0.5410, Training Accuracy: 0.8666, Validation Loss: 0.5710, Validation Accuracy: 0.8042\n",
      "No improvement in validation loss for 2 epochs.\n",
      "Epoch 27/40, Training Loss: 0.5399, Training Accuracy: 0.8744, Validation Loss: 0.5703, Validation Accuracy: 0.8120\n",
      "Validation loss decreased. Model saved to best_model_epoch_27.pth\n",
      "Epoch 28/40, Training Loss: 0.5395, Training Accuracy: 0.8692, Validation Loss: 0.5706, Validation Accuracy: 0.8120\n",
      "No improvement in validation loss for 1 epochs.\n",
      "Epoch 29/40, Training Loss: 0.5392, Training Accuracy: 0.8777, Validation Loss: 0.5709, Validation Accuracy: 0.8033\n",
      "No improvement in validation loss for 2 epochs.\n",
      "Epoch 30/40, Training Loss: 0.5399, Training Accuracy: 0.8777, Validation Loss: 0.5704, Validation Accuracy: 0.8138\n",
      "No improvement in validation loss for 3 epochs.\n",
      "Epoch 31/40, Training Loss: 0.5386, Training Accuracy: 0.8790, Validation Loss: 0.5703, Validation Accuracy: 0.8077\n",
      "No improvement in validation loss for 4 epochs.\n",
      "Epoch 32/40, Training Loss: 0.5387, Training Accuracy: 0.8766, Validation Loss: 0.5703, Validation Accuracy: 0.8094\n",
      "Validation loss decreased. Model saved to best_model_epoch_32.pth\n",
      "Epoch 33/40, Training Loss: 0.5386, Training Accuracy: 0.8777, Validation Loss: 0.5703, Validation Accuracy: 0.8077\n",
      "No improvement in validation loss for 1 epochs.\n",
      "Epoch 34/40, Training Loss: 0.5380, Training Accuracy: 0.8790, Validation Loss: 0.5703, Validation Accuracy: 0.8077\n",
      "No improvement in validation loss for 2 epochs.\n",
      "Epoch 35/40, Training Loss: 0.5387, Training Accuracy: 0.8759, Validation Loss: 0.5703, Validation Accuracy: 0.8085\n",
      "No improvement in validation loss for 3 epochs.\n",
      "Epoch 36/40, Training Loss: 0.5386, Training Accuracy: 0.8781, Validation Loss: 0.5703, Validation Accuracy: 0.8085\n",
      "No improvement in validation loss for 4 epochs.\n",
      "Epoch 37/40, Training Loss: 0.5385, Training Accuracy: 0.8785, Validation Loss: 0.5703, Validation Accuracy: 0.8085\n",
      "No improvement in validation loss for 5 epochs.\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Training Setup with wandb\n",
    "# ================================\n",
    "\n",
    "# Ensure that all necessary modules are imported\n",
    "import yaml\n",
    "import wandb\n",
    "import torch.optim as optim  # Import optim module\n",
    "\n",
    "# Load YAML configuration\n",
    "with open(r\"C:\\facultate an 3\\projects-simquery\\config.yaml\", \"r\") as file:\n",
    "    config_data = yaml.safe_load(file)\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"inferSent-project\",\n",
    "    config={\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"epochs\": config_data[\"model\"][\"parameters\"][\"epochs\"],\n",
    "        \"batch_size\": batch_size,\n",
    "        \"model_version\": 1,\n",
    "        \"word_emb_dim\": 300,\n",
    "        \"enc_lstm_dim\": 2048,\n",
    "        \"pool_type\": 'max',\n",
    "        \"dropout\": 0.5,  # Updated dropout rate\n",
    "        \"accuracy_threshold\": 0.5\n",
    "    },\n",
    "    name=config_data[\"model\"][\"name\"],\n",
    "    reinit=True\n",
    ")\n",
    "config = wandb.config\n",
    "\n",
    "num_epochs = config.epochs\n",
    "threshold = config.accuracy_threshold\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # Increased patience for early stopping\n",
    "trigger_times = 0\n",
    "\n",
    "# Instantiate the similarity model\n",
    "input_size = X1_train.shape[1]\n",
    "model_sim = SimilarityModel(input_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Changed loss function for binary classification\n",
    "optimizer = optim.Adam(model_sim.parameters(), lr=config.learning_rate, weight_decay=1e-5)  # Added weight decay\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                 factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_sim = model_sim.to(device)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "# ================================\n",
    "# Training and Validation Functions\n",
    "# ================================\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for x1_batch, x2_batch, y_batch in loader:\n",
    "        x1_batch = torch.tensor(x1_batch).to(device).float()\n",
    "        x2_batch = torch.tensor(x2_batch).to(device).float()\n",
    "        y_batch = torch.tensor(y_batch).to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x1_batch, x2_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x1_batch.size(0)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "        labels = y_batch.detach().cpu().numpy()\n",
    "        preds_binary = (preds >= threshold).astype(int)\n",
    "        labels_binary = (labels >= threshold).astype(int)\n",
    "        total_correct += (preds_binary == labels_binary).sum()\n",
    "\n",
    "        # Store predictions and labels\n",
    "        all_preds.extend(preds_binary)\n",
    "        all_labels.extend(labels_binary)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_accuracy = total_correct / len(loader.dataset)\n",
    "    \n",
    "    return avg_loss, avg_accuracy, all_preds, all_labels\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x1_batch, x2_batch, y_batch in loader:\n",
    "            x1_batch = torch.tensor(x1_batch).to(device).float()\n",
    "            x2_batch = torch.tensor(x2_batch).to(device).float()\n",
    "            y_batch = torch.tensor(y_batch).to(device).float()\n",
    "\n",
    "            outputs = model(x1_batch, x2_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item() * x1_batch.size(0)\n",
    "\n",
    "            preds = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "            labels = y_batch.detach().cpu().numpy()\n",
    "            preds_binary = (preds >= threshold).astype(int)\n",
    "            labels_binary = (labels >= threshold).astype(int)\n",
    "            total_correct += (preds_binary == labels_binary).sum()\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(preds_binary)\n",
    "            all_labels.extend(labels_binary)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_accuracy = total_correct / len(loader.dataset)\n",
    "    \n",
    "    return avg_loss, avg_accuracy, all_preds, all_labels\n",
    "\n",
    "# ================================\n",
    "# Training Loop\n",
    "# ================================\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc, train_preds, train_labels = train(model_sim, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_preds, val_labels = validate(model_sim, val_loader, criterion, device)\n",
    "\n",
    "    # Save metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}, \"\n",
    "          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_accuracy\": val_acc\n",
    "    })\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trigger_times = 0\n",
    "        model_path = f\"best_model_epoch_{epoch+1}.pth\"\n",
    "        torch.save(model_sim.state_dict(), model_path)\n",
    "        print(f\"Validation loss decreased. Model saved to {model_path}\")\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        print(f\"No improvement in validation loss for {trigger_times} epochs.\")\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'precision_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m conf_matrix \u001b[38;5;241m=\u001b[39m confusion_matrix(val_labels_binned, val_preds_binned)\n\u001b[0;32m      2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(val_labels_binned, val_preds_binned)\n\u001b[1;32m----> 3\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m(val_labels_binned, val_preds_binned, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(val_labels_binned, val_preds_binned, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(val_labels_binned, val_preds_binned, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'precision_score' is not defined"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(val_labels_binned, val_preds_binned)\n",
    "accuracy = accuracy_score(val_labels_binned, val_preds_binned)\n",
    "precision = precision_score(val_labels_binned, val_preds_binned, average='binary')\n",
    "recall = recall_score(val_labels_binned, val_preds_binned, average='binary')\n",
    "f1 = f1_score(val_labels_binned, val_preds_binned, average='binary')\n",
    "\n",
    "print(f\"Confusion Matrix Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Example 1:\n",
      "  Premise: The cat is on the mat.\n",
      "  Hypothesis: A cat is sitting on a rug.\n",
      "  True Score: 4.50\n",
      "  Predicted Score: 0.92\n",
      "  Error: 3.58\n",
      "----------------------------------------\n",
      "Example 2:\n",
      "  Premise: The sun is shining brightly.\n",
      "  Hypothesis: It is raining heavily.\n",
      "  True Score: 0.00\n",
      "  Predicted Score: 0.89\n",
      "  Error: 0.89\n",
      "----------------------------------------\n",
      "Example 3:\n",
      "  Premise: A man is playing the guitar.\n",
      "  Hypothesis: A person is strumming a musical instrument.\n",
      "  True Score: 4.80\n",
      "  Predicted Score: 1.08\n",
      "  Error: 3.72\n",
      "----------------------------------------\n",
      "Example 4:\n",
      "  Premise: She is reading a book.\n",
      "  Hypothesis: She is watching a movie.\n",
      "  True Score: 1.00\n",
      "  Predicted Score: 0.93\n",
      "  Error: 0.07\n",
      "----------------------------------------\n",
      "Example 5:\n",
      "  Premise: The car is red.\n",
      "  Hypothesis: The vehicle is blue.\n",
      "  True Score: 0.50\n",
      "  Predicted Score: 0.82\n",
      "  Error: 0.32\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oana\\AppData\\Local\\Temp\\ipykernel_7828\\1142258386.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  test_premises_embedded = torch.tensor([embeddings_1[i % len(embeddings_1)] for i in range(len(test_premises))]).float()\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    {\"premise\": \"The cat is on the mat.\", \"hypothesis\": \"A cat is sitting on a rug.\", \"true_score\": 4.5},\n",
    "    {\"premise\": \"The sun is shining brightly.\", \"hypothesis\": \"It is raining heavily.\", \"true_score\": 0.0},\n",
    "    {\"premise\": \"A man is playing the guitar.\", \"hypothesis\": \"A person is strumming a musical instrument.\", \"true_score\": 4.8},\n",
    "    {\"premise\": \"She is reading a book.\", \"hypothesis\": \"She is watching a movie.\", \"true_score\": 1.0},\n",
    "    {\"premise\": \"The car is red.\", \"hypothesis\": \"The vehicle is blue.\", \"true_score\": 0.5}\n",
    "]\n",
    "\n",
    "# Convert test data into tensors (simulate preprocessing real test data)\n",
    "test_premises = [example[\"premise\"] for example in test_data]\n",
    "test_hypotheses = [example[\"hypothesis\"] for example in test_data]\n",
    "true_scores = torch.tensor([example[\"true_score\"] for example in test_data]).float()\n",
    "\n",
    "# Tokenize and embed the test premises and hypotheses (replace with your embedding method)\n",
    "# For demonstration, random embeddings are used (replace with actual embeddings)\n",
    "test_premises_embedded = torch.tensor([embeddings_1[i % len(embeddings_1)] for i in range(len(test_premises))]).float()\n",
    "test_hypotheses_embedded = torch.tensor([embeddings_1[i % len(embeddings_1)] for i in range(len(test_hypotheses))]).float()\n",
    "\n",
    "# Move to the appropriate device\n",
    "test_premises_embedded = test_premises_embedded.to(device)\n",
    "test_hypotheses_embedded = test_hypotheses_embedded.to(device)\n",
    "true_scores = true_scores.to(device)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "model_sim.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_scores = model_sim(test_premises_embedded, test_hypotheses_embedded).squeeze()\n",
    "\n",
    "# Compare predictions with true scores\n",
    "print(\"Test Results:\")\n",
    "for i, (premise, hypothesis, true_score, predicted_score) in enumerate(zip(test_premises, test_hypotheses, true_scores, predicted_scores)):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Premise: {premise}\")\n",
    "    print(f\"  Hypothesis: {hypothesis}\")\n",
    "    print(f\"  True Score: {true_score.item():.2f}\")\n",
    "    print(f\"  Predicted Score: {predicted_score.item():.2f}\")\n",
    "    print(f\"  Error: {abs(true_score.item() - predicted_score.item()):.2f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "  Premise: The cat is on the mat.\n",
      "  Hypothesis: A cat is sitting on a rug.\n",
      "  True Label: 1.0\n",
      "  Predicted Label: 1.0\n",
      "  Correct\n",
      "----------------------------------------\n",
      "Example 2:\n",
      "  Premise: The sun is shining brightly.\n",
      "  Hypothesis: It is raining heavily.\n",
      "  True Label: 0.0\n",
      "  Predicted Label: 1.0\n",
      "  Incorrect\n",
      "----------------------------------------\n",
      "Example 3:\n",
      "  Premise: A man is playing the guitar.\n",
      "  Hypothesis: A person is strumming a musical instrument.\n",
      "  True Label: 1.0\n",
      "  Predicted Label: 1.0\n",
      "  Correct\n",
      "----------------------------------------\n",
      "Example 4:\n",
      "  Premise: She is reading a book.\n",
      "  Hypothesis: She is watching a movie.\n",
      "  True Label: 0.0\n",
      "  Predicted Label: 1.0\n",
      "  Incorrect\n",
      "----------------------------------------\n",
      "Example 5:\n",
      "  Premise: The car is red.\n",
      "  Hypothesis: The vehicle is blue.\n",
      "  True Label: 0.0\n",
      "  Predicted Label: 1.0\n",
      "  Incorrect\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    {\"premise\": \"The cat is on the mat.\", \"hypothesis\": \"A cat is sitting on a rug.\", \"true_label\": 1},\n",
    "    {\"premise\": \"The sun is shining brightly.\", \"hypothesis\": \"It is raining heavily.\", \"true_label\": 0},\n",
    "    {\"premise\": \"A man is playing the guitar.\", \"hypothesis\": \"A person is strumming a musical instrument.\", \"true_label\": 1},\n",
    "    {\"premise\": \"She is reading a book.\", \"hypothesis\": \"She is watching a movie.\", \"true_label\": 0},\n",
    "    {\"premise\": \"The car is red.\", \"hypothesis\": \"The vehicle is blue.\", \"true_label\": 0}\n",
    "]\n",
    "\n",
    "test_premises = [example[\"premise\"] for example in test_data]\n",
    "test_hypotheses = [example[\"hypothesis\"] for example in test_data]\n",
    "true_labels = torch.tensor([example[\"true_label\"] for example in test_data]).float()\n",
    "\n",
    "\n",
    "test_premises_embedded = torch.tensor([embeddings_1[i % len(embeddings_1)] for i in range(len(test_premises))]).float()\n",
    "test_hypotheses_embedded = torch.tensor([embeddings_1[i % len(embeddings_1)] for i in range(len(test_hypotheses))]).float()\n",
    "\n",
    "\n",
    "test_premises_embedded = test_premises_embedded.to(device)\n",
    "test_hypotheses_embedded = test_hypotheses_embedded.to(device)\n",
    "true_labels = true_labels.to(device)\n",
    "\n",
    "model_sim.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model_sim(test_premises_embedded, test_hypotheses_embedded)\n",
    "    predicted_labels = (predictions >= threshold).float()\n",
    "\n",
    "for i, (premise, hypothesis, true_label, predicted_label) in enumerate(zip(test_premises, test_hypotheses, true_labels, predicted_labels)):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Premise: {premise}\")\n",
    "    print(f\"  Hypothesis: {hypothesis}\")\n",
    "    print(f\"  True Label: {true_label.item()}\")\n",
    "    print(f\"  Predicted Label: {predicted_label.item()}\")\n",
    "    print(f\"  {'Correct' if true_label.item() == predicted_label.item() else 'Incorrect'}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
