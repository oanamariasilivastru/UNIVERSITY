{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_sentence_spacy(sentence, tokenize=True):\n",
    "    if tokenize:\n",
    "        doc = nlp(sentence.lower())\n",
    "        return [token.text for token in doc]\n",
    "    else:\n",
    "        return sentence.lower().split()\n",
    "    \n",
    "# Definirea clasei InferSent cu metoda preprocess_sentence_spacy\n",
    "class InferSent(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(InferSent, self).__init__()\n",
    "        self.bsize = config['bsize']\n",
    "        self.word_emb_dim = config['word_emb_dim']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.pool_type = config['pool_type']\n",
    "        self.dpout_model = config['dpout_model']\n",
    "        self.version = 1 if 'version' not in config else config['version']\n",
    "\n",
    "        self.enc_lstm = nn.LSTM(\n",
    "            input_size=self.word_emb_dim,\n",
    "            hidden_size=self.enc_lstm_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            dropout=self.dpout_model\n",
    "        )\n",
    "\n",
    "        assert self.version in [1, 2]\n",
    "        if self.version == 1:\n",
    "            self.bos = '<s>'\n",
    "            self.eos = '</s>'\n",
    "            self.max_pad = True\n",
    "            self.moses_tok = False\n",
    "        elif self.version == 2:\n",
    "            self.bos = '<p>'\n",
    "            self.eos = '</p>'\n",
    "            self.max_pad = False\n",
    "            self.moses_tok = True\n",
    "\n",
    "        # Strat de regresie pentru similaritate\n",
    "        self.regressor = nn.Linear(2 * self.enc_lstm_dim * 2, 1)  # 2*lstm_dim * 2 pentru concatenarea premisei și hypothesis\n",
    "\n",
    "        # Încarcă modelul spaCy o singură dată\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_sentence_spacy(sentence, tokenize=True):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        if tokenize:\n",
    "            doc = nlp(sentence.lower())\n",
    "            return [token.text for token in doc]\n",
    "        else:\n",
    "            return sentence.lower().split()\n",
    "\n",
    "    def is_cuda(self):\n",
    "        # Verifică dacă modelul este pe GPU\n",
    "        return next(self.parameters()).is_cuda\n",
    "\n",
    "    def forward(self, sent_tuple):\n",
    "        sent, sent_len = sent_tuple  # sent: (batch_size, seq_len, emb_dim); sent_len: (batch_size)\n",
    "\n",
    "        # Sort by length in descending order\n",
    "        sent_len_sorted, idx_sort = torch.sort(sent_len, descending=True)\n",
    "        sent_sorted = sent.index_select(0, idx_sort)\n",
    "\n",
    "        # Pack the sequence\n",
    "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent_sorted, sent_len_sorted.cpu(), batch_first=True, enforce_sorted=True)\n",
    "        sent_output_packed, _ = self.enc_lstm(sent_packed)  # sent_output_packed: PackedSequence; _ : hidden state\n",
    "\n",
    "        # Unpack the sequence\n",
    "        sent_output, _ = nn.utils.rnn.pad_packed_sequence(sent_output_packed, batch_first=True)  # sent_output: (batch_size, seq_len, 2*enc_lstm_dim)\n",
    "\n",
    "        # Unsort to original order\n",
    "        _, idx_unsort = torch.sort(idx_sort, descending=False)\n",
    "        sent_output = sent_output.index_select(0, idx_unsort)\n",
    "        sent_len_sorted = sent_len_sorted.index_select(0, idx_unsort)\n",
    "\n",
    "        # Pooling\n",
    "        if self.pool_type == \"mean\":\n",
    "            # Calculate the sum over the sequence dimension and divide by the lengths\n",
    "            sent_len_unsorted = sent_len_sorted.float().unsqueeze(1)  # (batch_size, 1)\n",
    "            emb = torch.sum(sent_output, dim=1) / sent_len_unsorted  # (batch_size, 2*enc_lstm_dim)\n",
    "        elif self.pool_type == \"max\":\n",
    "            if not self.max_pad:\n",
    "                sent_output[sent_output == 0] = -1e9\n",
    "            emb, _ = torch.max(sent_output, dim=1)  # (batch_size, 2*enc_lstm_dim)\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def encode_sentence_pair(self, premise_tuple, hypothesis_tuple):\n",
    "        encoded_premise = self.forward(premise_tuple)\n",
    "        encoded_hypothesis = self.forward(hypothesis_tuple)\n",
    "        combined = torch.cat((encoded_premise, encoded_hypothesis), dim=1)  # (batch_size, 4*enc_lstm_dim)\n",
    "        return combined\n",
    "\n",
    "    def regress_similarity(self, combined):\n",
    "        similarity = self.regressor(combined)\n",
    "        return similarity.squeeze(1)  # Returnează un tensor de dimensiune (batch_size)\n",
    "\n",
    "    def set_w2v_path(self, w2v_path):\n",
    "        self.w2v_path = w2v_path\n",
    "\n",
    "    def get_word_dict(self, sentences, tokenize=True):\n",
    "        # Crează vocabularul de cuvinte\n",
    "        word_dict = {}\n",
    "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
    "        for sent in sentences:\n",
    "            for word in sent:\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = ''\n",
    "        word_dict[self.bos] = ''\n",
    "        word_dict[self.eos] = ''\n",
    "        return word_dict\n",
    "\n",
    "    def get_w2v(self, word_dict):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # Crează word_vec cu vectori w2v\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                split_line = line.strip().split(' ')\n",
    "                word = split_line[0]\n",
    "                vec = np.array([float(val) for val in split_line[1:]], dtype='float32')\n",
    "                if word in word_dict:\n",
    "                    word_vec[word] = vec\n",
    "        print('Found %s/%s words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
    "        return word_vec\n",
    "\n",
    "    def get_w2v_k(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # Crează word_vec cu primii K vectori w2v\n",
    "        k = 0\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
    "                    break\n",
    "                split_line = line.strip().split(' ')\n",
    "                word = split_line[0]\n",
    "                vec = np.array([float(val) for val in split_line[1:]], dtype='float32')\n",
    "                if k <= K or word in [self.bos, self.eos]:\n",
    "                    word_vec[word] = vec\n",
    "                    k += 1\n",
    "        return word_vec\n",
    "\n",
    "    def build_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "        self.word_vec = self.get_w2v(word_dict)\n",
    "        print('Vocab size : %s' % (len(self.word_vec)))\n",
    "\n",
    "    def build_vocab_k_words(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        self.word_vec = self.get_w2v_k(K)\n",
    "        print('Vocab size : %s' % (len(self.word_vec)))\n",
    "\n",
    "    def update_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
    "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "\n",
    "        # Păstrează doar cuvintele noi\n",
    "        new_words = {word: '' for word in word_dict if word not in self.word_vec}\n",
    "        if new_words:\n",
    "            new_word_vec = self.get_w2v(new_words)\n",
    "            self.word_vec.update(new_word_vec)\n",
    "            print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
    "        else:\n",
    "            print('No new words to add.')\n",
    "\n",
    "    def get_batch(self, batch):\n",
    "        # Determină dimensiunile batch-ului\n",
    "        batch_size = len(batch)\n",
    "        max_len = max(len(s) for s in batch)\n",
    "        \n",
    "        # Inițializează un array de numpy pentru embed\n",
    "        embed = np.zeros((batch_size, max_len, self.word_emb_dim), dtype=np.float32)\n",
    "        \n",
    "        for i, sentence in enumerate(batch):\n",
    "            for j, word in enumerate(sentence):\n",
    "                embed[i, j, :] = self.word_vec.get(word, self.word_vec.get('<unk>', np.zeros(self.word_emb_dim)))\n",
    "        \n",
    "        return torch.from_numpy(embed)  # Returnează un tensor PyTorch\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        # Folosește metoda preprocess_sentence_spacy pentru tokenizare\n",
    "        return self.preprocess_sentence_spacy(s)\n",
    "\n",
    "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
    "        # Adaugă <s> și </s> și tokenizează dacă este necesar\n",
    "        sentences = [\n",
    "            [self.bos] + s + [self.eos] for s in sentences\n",
    "        ]\n",
    "        n_w = np.sum([len(x) for x in sentences])\n",
    "\n",
    "        # Filtrează cuvintele fără vectori w2v\n",
    "        for i in range(len(sentences)):\n",
    "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
    "            if not s_f:\n",
    "                import warnings\n",
    "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. Replacing by \"</s>\"..' % (sentences[i], i))\n",
    "                s_f = [self.eos]\n",
    "            sentences[i] = s_f\n",
    "\n",
    "        lengths = np.array([len(s) for s in sentences])\n",
    "        n_wk = np.sum(lengths)\n",
    "        if verbose:\n",
    "            print('Nb words kept : %s/%s (%.1f%%)' % (n_wk, n_w, 100.0 * n_wk / n_w))\n",
    "\n",
    "        # Sortează propozițiile după lungime descrescătoare\n",
    "        lengths_sorted, idx_sort = torch.sort(torch.tensor(lengths), descending=True)\n",
    "        sentences_sorted = [sentences[i] for i in idx_sort]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"First 5 sorted sentence lengths: {lengths_sorted[:5].tolist()}\")\n",
    "            print(f\"First 5 sorted sentences: {sentences_sorted[:5]}\")\n",
    "\n",
    "        return sentences_sorted, lengths_sorted.numpy(), idx_sort.numpy()\n",
    "\n",
    "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
    "        tic = time.time()\n",
    "        sentences_sorted, lengths_sorted, idx_sort = self.prepare_samples(\n",
    "                        sentences, bsize, tokenize, verbose)\n",
    "\n",
    "        embeddings = []\n",
    "        for stidx in range(0, len(sentences_sorted), bsize):\n",
    "            batch_sentences = sentences_sorted[stidx:stidx + bsize]\n",
    "            batch = self.get_batch(batch_sentences)\n",
    "            if self.is_cuda():\n",
    "                batch = batch.cuda()\n",
    "            lengths = lengths_sorted[stidx:stidx + bsize]\n",
    "            with torch.no_grad():\n",
    "                batch_output = self.forward((batch, torch.tensor(lengths).cuda() if self.is_cuda() else torch.tensor(lengths)))\n",
    "                embeddings.append(batch_output.cpu().numpy())\n",
    "        embeddings = np.vstack(embeddings)\n",
    "\n",
    "        # Unsort\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        embeddings = embeddings[idx_unsort]\n",
    "\n",
    "        if verbose:\n",
    "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
    "                    len(embeddings)/(time.time()-tic),\n",
    "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
    "        return embeddings\n",
    "\n",
    "    def visualize(self, sent, tokenize=True):\n",
    "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
    "        sent = [self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]\n",
    "\n",
    "        if len(sent) == 2 and sent[0] == self.bos and sent[1] == self.eos:\n",
    "            import warnings\n",
    "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
    "        \n",
    "        batch = self.get_batch([sent])\n",
    "        \n",
    "        if self.is_cuda():\n",
    "            batch = batch.cuda()\n",
    "        output = self.forward((batch, torch.tensor([len(sent)]).cuda() if self.is_cuda() else torch.tensor([len(sent)])))\n",
    "        output, idxs = torch.max(output, dim=1)\n",
    "        \n",
    "        idxs = idxs.cpu().numpy()\n",
    "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent))]\n",
    "\n",
    "        # visualize model\n",
    "        x = range(len(sent))\n",
    "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
    "        plt.xticks(x, sent, rotation=45)\n",
    "        plt.bar(x, y)\n",
    "        plt.ylabel('%')\n",
    "        plt.title('Visualisation of words importance')\n",
    "        plt.show()\n",
    "\n",
    "        return output, idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mset_w2v_path(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfacultate an 3\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprojects-simquery\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mword2vec.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Înlocuiește cu calea corectă\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Construirea vocabularului\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msentences_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Tokenizează propozițiile\u001b[39;00m\n\u001b[0;32m     39\u001b[0m sentences_1 \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mtokenize(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences_1]\n",
      "Cell \u001b[1;32mIn[6], line 160\u001b[0m, in \u001b[0;36mInferSent.build_vocab\u001b[1;34m(self, sentences, tokenize)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2v_path\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2v path not set\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 160\u001b[0m     word_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_word_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_w2v(word_dict)\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVocab size : \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_vec)))\n",
      "Cell \u001b[1;32mIn[6], line 118\u001b[0m, in \u001b[0;36mInferSent.get_word_dict\u001b[1;34m(self, sentences, tokenize)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_word_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Crează vocabularul de cuvinte\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     word_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 118\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [s\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tokenize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent:\n",
      "Cell \u001b[1;32mIn[6], line 199\u001b[0m, in \u001b[0;36mInferSent.tokenize\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, s):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# Folosește metoda preprocess_sentence_spacy pentru tokenizare\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_sentence_spacy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 60\u001b[0m, in \u001b[0;36mInferSent.preprocess_sentence_spacy\u001b[1;34m(sentence, tokenize)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_sentence_spacy\u001b[39m(sentence, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 60\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenize:\n\u001b[0;32m     62\u001b[0m         doc \u001b[38;5;241m=\u001b[39m nlp(sentence\u001b[38;5;241m.\u001b[39mlower())\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\spacy\\util.py:465\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\spacy\\util.py:501\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mname (str): The package name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[1;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\en_core_web_sm\\__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_init_py\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\spacy\\util.py:682\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[1;32m--> 682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\spacy\\util.py:547\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    538\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[0;32m    539\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[0;32m    540\u001b[0m     config,\n\u001b[0;32m    541\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[0;32m    546\u001b[0m )\n\u001b[1;32m--> 547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\spacy\\language.py:2246\u001b[0m, in \u001b[0;36mLanguage.from_disk\u001b[1;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m   2244\u001b[0m     \u001b[38;5;66;03m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[0;32m   2245\u001b[0m     exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(exclude) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m-> 2246\u001b[0m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserializers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   2247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m path  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m   2248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_link_components()\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\spacy\\util.py:1390\u001b[0m, in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, reader \u001b[38;5;129;01min\u001b[39;00m readers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1388\u001b[0m     \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:\n\u001b[1;32m-> 1390\u001b[0m         \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\spacy\\language.py:2240\u001b[0m, in \u001b[0;36mLanguage.from_disk.<locals>.<lambda>\u001b[1;34m(p, proc)\u001b[0m\n\u001b[0;32m   2238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_disk\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2239\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 2240\u001b[0m     deserializers[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m p, proc\u001b[38;5;241m=\u001b[39mproc: \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[misc]\u001b[39;49;00m\n\u001b[0;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvocab\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m   2244\u001b[0m     \u001b[38;5;66;03m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[0;32m   2245\u001b[0m     exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(exclude) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:343\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.from_disk\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\spacy\\util.py:1390\u001b[0m, in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, reader \u001b[38;5;129;01min\u001b[39;00m readers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1388\u001b[0m     \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:\n\u001b[1;32m-> 1390\u001b[0m         \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:333\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.from_disk.load_model\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:334\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.from_disk.load_model\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\thinc\\model.py:638\u001b[0m, in \u001b[0;36mModel.from_bytes\u001b[1;34m(self, bytes_data)\u001b[0m\n\u001b[0;32m    636\u001b[0m msg \u001b[38;5;241m=\u001b[39m srsly\u001b[38;5;241m.\u001b[39mmsgpack_loads(bytes_data)\n\u001b[0;32m    637\u001b[0m msg \u001b[38;5;241m=\u001b[39m convert_recursive(is_xp_array, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39masarray, msg)\n\u001b[1;32m--> 638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\facultate an 3\\projects-simquery\\.venv\\Lib\\site-packages\\thinc\\model.py:673\u001b[0m, in \u001b[0;36mModel.from_dict\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name, value \u001b[38;5;129;01min\u001b[39;00m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 673\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    674\u001b[0m     node\u001b[38;5;241m.\u001b[39mset_param(param_name, value)\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, shim_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshims\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "# Asigură-te că ai definit funcția preprocess_sentence_spacy și clasa InferSent înainte de acest punct\n",
    "\n",
    "# Încarcă setul de date\n",
    "data_path = r'C:\\facultate an 3\\projects-simquery\\data\\sts_train.csv'  # Asigură-te că calea este corectă\n",
    "data = pd.read_csv(data_path, delimiter='\\t')\n",
    "\n",
    "# Filtrează datele pentru a elimina rândurile cu valori lipsă\n",
    "data = data.dropna(subset=['sent_1', 'sent_2', 'sim'])\n",
    "\n",
    "# Extrage propozițiile și scorurile\n",
    "sentences_1 = data['sent_1'].tolist()\n",
    "sentences_2 = data['sent_2'].tolist()\n",
    "similarities = data['sim'].tolist()\n",
    "\n",
    "# Definirea dicționarului de configurare\n",
    "config = {\n",
    "    'bsize': 64,\n",
    "    'word_emb_dim': 300,\n",
    "    'enc_lstm_dim': 2048,\n",
    "    'pool_type': 'mean',  # sau 'max'\n",
    "    'dpout_model': 0.0,\n",
    "    'version': 1\n",
    "}\n",
    "\n",
    "# Instanțierea modelului\n",
    "model = InferSent(config)\n",
    "\n",
    "# Setarea căii către vectorii word2vec pre-antrenați\n",
    "model.set_w2v_path(r'C:\\facultate an 3\\projects-simquery\\data\\word2vec.txt')  # Înlocuiește cu calea corectă\n",
    "\n",
    "# Construirea vocabularului\n",
    "model.build_vocab(sentences_1 + sentences_2, tokenize=True)\n",
    "\n",
    "# Tokenizează propozițiile\n",
    "sentences_1 = [model.tokenize(s) for s in sentences_1]\n",
    "sentences_2 = [model.tokenize(s) for s in sentences_2]\n",
    "\n",
    "print(\"Tokenization successful using spaCy!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
