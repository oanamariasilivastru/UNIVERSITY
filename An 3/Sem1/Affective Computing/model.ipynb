{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Oana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Starting video stream. Press 'q' to exit.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from mtcnn import MTCNN\n",
    "from deepface import DeepFace\n",
    "import torch\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # Set backend before importing pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Funcții de preprocesare a imaginii\n",
    "def apply_clahe(img):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    hsv[:, :, 2] = clahe.apply(hsv[:, :, 2])\n",
    "    img_clahe = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "    return img_clahe\n",
    "\n",
    "def apply_gamma_correction(img, gamma=1.5):\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "                      for i in np.arange(256)]).astype(\"uint8\")\n",
    "    img_gamma = cv2.LUT(img, table)\n",
    "    return img_gamma\n",
    "\n",
    "# Verifică dacă GPU-ul este disponibil\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initializează detectorul MTCNN fără puncte cheie\n",
    "detector = MTCNN()\n",
    "\n",
    "# Inițializează captura video\n",
    "cap = cv2.VideoCapture(0)  # 0 pentru camera implicită\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# Definirea emoțiilor în engleză (conform FER Plus)\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Încărcarea modelului de detectare a emoțiilor\n",
    "model_path = 'model_emotion.h5'  # Asigură-te că ai modelul în acest path\n",
    "try:\n",
    "    model = load_model(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Starting video stream. Press 'q' to exit.\")\n",
    "\n",
    "# Initialize Matplotlib pentru plotare în timp real\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots()\n",
    "emotion_counts = {label: 0 for label in emotion_labels}\n",
    "bars = ax.bar(emotion_labels, [0]*len(emotion_labels), color='skyblue')\n",
    "ax.set_ylim(0, 10)  # Limită inițială a axei Y\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Emotion Counts Over Time')\n",
    "plt.show()\n",
    "\n",
    "frame_counter = 0\n",
    "start_time = time.time()\n",
    "\n",
    "def get_largest_face(faces):\n",
    "    if not faces:\n",
    "        return None\n",
    "    largest_face = max(faces, key=lambda face: face['box'][2] * face['box'][3])\n",
    "    return largest_face\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read frame from camera.\")\n",
    "        break\n",
    "\n",
    "    # Aplică preprocesarea: CLAHE și corecție gamma\n",
    "    processed_frame = apply_clahe(frame)\n",
    "    processed_frame = apply_gamma_correction(processed_frame, gamma=1.5)\n",
    "\n",
    "    # Detectare fețe cu MTCNN\n",
    "    faces = detector.detect_faces(processed_frame)\n",
    "    largest_face = get_largest_face(faces)\n",
    "\n",
    "    # Creare mască pentru a blura fundalul\n",
    "    mask = np.zeros_like(processed_frame)\n",
    "    if largest_face:\n",
    "        x, y, w, h = largest_face['box']\n",
    "        x, y = max(0, x), max(0, y)\n",
    "        w, h = min(w, processed_frame.shape[1] - x), min(h, processed_frame.shape[0] - y)\n",
    "        cv2.rectangle(mask, (x, y), (x + w, y + h), (255, 255, 255), -1)\n",
    "\n",
    "    # Aplicare blur pe fundal\n",
    "    blurred_frame = cv2.GaussianBlur(processed_frame, (21, 21), 0)\n",
    "    # Combina blurred_frame și processed_frame folosind masca\n",
    "    combined_frame = np.where(mask == np.array([255, 255, 255]), processed_frame, blurred_frame)\n",
    "\n",
    "    current_time = time.time()\n",
    "    if current_time - start_time >= 1:\n",
    "        if largest_face:\n",
    "            try:\n",
    "                face_roi_original = combined_frame[y:y + h, x:x + w].copy()\n",
    "                face_gray = cv2.cvtColor(face_roi_original, cv2.COLOR_BGR2GRAY)\n",
    "                face_eq = cv2.equalizeHist(face_gray)\n",
    "                face_eq = cv2.GaussianBlur(face_eq, (3, 3), 0)\n",
    "                face_roi = cv2.resize(face_eq, (48, 48))\n",
    "                face_roi = face_roi.astype('float32') / 255.0\n",
    "                face_roi = np.expand_dims(face_roi, axis=-1)\n",
    "                face_roi = np.expand_dims(face_roi, axis=0)\n",
    "\n",
    "                # Predicție emoție folosind modelul antrenat\n",
    "                predictions = model.predict(face_roi)[0]\n",
    "                if not isinstance(predictions, np.ndarray):\n",
    "                    predictions = np.array(predictions)\n",
    "                predictions = predictions / np.sum(predictions)\n",
    "\n",
    "                # Predicție emoție folosind DeepFace\n",
    "                face_roi_rgb = cv2.cvtColor(face_roi_original, cv2.COLOR_BGR2RGB)\n",
    "                deepface_result = DeepFace.analyze(face_roi_rgb, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "                # Extrage probabilitățile emoțiilor din DeepFace\n",
    "                if isinstance(deepface_result, list):\n",
    "                    if len(deepface_result) > 0:\n",
    "                        deepface_emotions = deepface_result[0].get('emotion', {})\n",
    "                    else:\n",
    "                        deepface_emotions = {}\n",
    "                elif isinstance(deepface_result, dict):\n",
    "                    deepface_emotions = deepface_result.get('emotion', {})\n",
    "                else:\n",
    "                    deepface_emotions = {}\n",
    "\n",
    "                # Maparea etichetelor DeepFace la cele din modelul tău personalizat\n",
    "                deepface_emotions = {k.capitalize(): v for k, v in deepface_emotions.items()}\n",
    "                deepface_mapped = {\n",
    "                    'Angry': deepface_emotions.get('Angry', 0.0),\n",
    "                    'Disgust': deepface_emotions.get('Disgust', 0.0),\n",
    "                    'Fear': deepface_emotions.get('Fear', 0.0),\n",
    "                    'Happy': deepface_emotions.get('Happy', 0.0),\n",
    "                    'Sad': deepface_emotions.get('Sad', 0.0),\n",
    "                    'Surprise': deepface_emotions.get('Surprise', 0.0),\n",
    "                    'Neutral': deepface_emotions.get('Neutral', 0.0)\n",
    "                }\n",
    "\n",
    "                deepface_predictions = np.array([deepface_mapped[label] for label in emotion_labels])\n",
    "                if np.sum(deepface_predictions) > 0:\n",
    "                    deepface_predictions = deepface_predictions / np.sum(deepface_predictions)\n",
    "                else:\n",
    "                    deepface_predictions = np.zeros(len(emotion_labels))\n",
    "\n",
    "                # Calcularea mediei probabilităților\n",
    "                combined_predictions = (predictions + deepface_predictions) / 2\n",
    "                final_max_index = np.argmax(combined_predictions)\n",
    "                final_emotion = emotion_labels[final_max_index]\n",
    "                emotion_counts[final_emotion] += 1\n",
    "\n",
    "                # Desenare dreptunghi pe față fără acuratețe\n",
    "                color = (255, 0, 0)  # Albastru\n",
    "                cv2.rectangle(combined_frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(combined_frame, final_emotion, (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "                # Actualizare grafic\n",
    "                for bar, label in zip(bars, emotion_labels):\n",
    "                    bar.set_height(emotion_counts[label])\n",
    "                max_count = max(emotion_counts.values()) if emotion_counts.values() else 10\n",
    "                ax.set_ylim(0, max(max_count + 5, 10))\n",
    "                fig.canvas.draw()\n",
    "                fig.canvas.flush_events()\n",
    "                plt.pause(0.001)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Error processing face:\", e)\n",
    "                pass\n",
    "\n",
    "        start_time = current_time\n",
    "\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Processed Frame - Blurred Background and Emotion', combined_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SECOND APPRPOACH\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # Set the Matplotlib backend before importing pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. CUSTOM IMPORTS FOR YOUR MODELS\n",
    "# -------------------------------------------------------------------------\n",
    "# Assume you have:\n",
    "#   - face_model.py (a PyTorch face detection model pretrained on DarkFace)\n",
    "#   - emotion_model.py (a PyTorch emotion recognition model)\n",
    "#\n",
    "# Import them here. For example:\n",
    "#\n",
    "# from face_model import MyDarkFaceDetector  # your custom class\n",
    "# from emotion_model import MyEmotionModel   # your custom class\n",
    "#\n",
    "# In this demo, we'll just define placeholders below for illustration.\n",
    "\n",
    "# Placeholder classes (replace with your actual implementations!)\n",
    "class MyDarkFaceDetector(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Example: A face detection model pretrained on DarkFace dataset.\n",
    "    Must return 'boxes' and 'scores' in forward() for each image.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define your architecture\n",
    "\n",
    "    def forward(self, images):\n",
    "        # images: [batch_size, 3, H, W] (torch.Tensor)\n",
    "        # Return a list[dict], each dict with 'boxes' & 'scores'\n",
    "        # boxes: [N, 4], scores: [N] ...\n",
    "        return [{\n",
    "            'boxes': torch.tensor([[100, 100, 200, 200]]),\n",
    "            'scores': torch.tensor([0.99])\n",
    "        }]\n",
    "\n",
    "\n",
    "class MyEmotionModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Example: An emotion recognition model\n",
    "    input -> ... -> output logits for each emotion class\n",
    "    \"\"\"\n",
    "    def __init__(self, num_emotions=7):\n",
    "        super().__init__()\n",
    "        # define your architecture\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, 1 (or 3), H, W], e.g., 48x48 if grayscale\n",
    "        # Return logits for each emotion\n",
    "        return torch.randn(x.size(0), 7)  # random placeholder\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. IMAGE PREPROCESSING FUNCTIONS\n",
    "# -------------------------------------------------------------------------\n",
    "def apply_clahe(img):\n",
    "    \"\"\"\n",
    "    Applies CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    to the Value channel in HSV space, which can enhance local contrast.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    hsv[:, :, 2] = clahe.apply(hsv[:, :, 2])  # apply on V channel\n",
    "    img_clahe = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    return img_clahe\n",
    "\n",
    "def apply_gamma_correction(img, gamma=1.5):\n",
    "    \"\"\"\n",
    "    Applies gamma correction to adjust brightness in a non-linear way.\n",
    "    gamma > 1.0 darkens the image, gamma < 1.0 lightens it.\n",
    "    \"\"\"\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([\n",
    "        ((i / 255.0) ** invGamma) * 255 for i in np.arange(256)\n",
    "    ]).astype(\"uint8\")\n",
    "    return cv2.LUT(img, table)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. UTILITY FUNCTIONS\n",
    "# -------------------------------------------------------------------------\n",
    "def largest_box(boxes, scores, score_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Given 'boxes' (N,4) and 'scores' (N), returns the bounding box\n",
    "    with the largest area among those that exceed 'score_threshold'.\n",
    "    Each box is [x_min, y_min, x_max, y_max].\n",
    "    Returns None if no box meets the threshold.\n",
    "    \"\"\"\n",
    "    valid_indices = (scores >= score_threshold).nonzero(as_tuple=True)[0]\n",
    "    if len(valid_indices) == 0:\n",
    "        return None\n",
    "    \n",
    "    # compute areas\n",
    "    areas = []\n",
    "    for i in valid_indices:\n",
    "        x1, y1, x2, y2 = boxes[i]\n",
    "        areas.append((x2 - x1) * (y2 - y1))\n",
    "    \n",
    "    max_idx = valid_indices[torch.argmax(torch.tensor(areas))]\n",
    "    return boxes[max_idx].tolist()  # convert to Python list\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. LOAD YOUR PRETRAINED MODELS (FACE DETECTOR & EMOTION MODEL)\n",
    "# -------------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Replace these with your real model definitions & weights\n",
    "face_detector = MyDarkFaceDetector().to(device)\n",
    "emotion_model = MyEmotionModel(num_emotions=7).to(device)\n",
    "\n",
    "# Example: load state dicts from files\n",
    "# face_detector.load_state_dict(torch.load(\"face_detector_darkface.pth\", map_location=device))\n",
    "# emotion_model.load_state_dict(torch.load(\"emotion_model.pth\", map_location=device))\n",
    "\n",
    "face_detector.eval()\n",
    "emotion_model.eval()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. SET UP WEBCAM CAPTURE\n",
    "# -------------------------------------------------------------------------\n",
    "cap = cv2.VideoCapture(0)  # 0 = default webcam\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# Define your emotion labels (English)\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# MATPLOTLIB FOR REAL-TIME BAR CHART\n",
    "# -------------------------------------------------------------------------\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots()\n",
    "emotion_counts = {label: 0 for label in emotion_labels}\n",
    "bars = ax.bar(emotion_labels, [0]*len(emotion_labels), color='skyblue')\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Emotion Counts Over Time')\n",
    "plt.show()\n",
    "\n",
    "start_time = time.time()\n",
    "update_interval = 1.0  # seconds\n",
    "\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read frame from camera.\")\n",
    "        break\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 6. IMAGE PREPROCESSING (CLAHE, GAMMA)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Convert to BGR if needed; usually cap.read() is already BGR\n",
    "    processed_frame = apply_clahe(frame)\n",
    "    processed_frame = apply_gamma_correction(processed_frame, gamma=1.5)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 7. FACE DETECTION WITH CUSTOM MODEL (DARKFACE PRETRAINED)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Prepare the frame for your face detector:\n",
    "    # Usually you do transforms, e.g. ToTensor(), etc.\n",
    "    # We'll do a minimal approach here:\n",
    "    input_tensor = torch.from_numpy(processed_frame.copy()).permute(2, 0, 1).float().unsqueeze(0)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        detections = face_detector(input_tensor)  # expected to return list of dicts\n",
    "\n",
    "    # We assume the first element of detections is a dict with 'boxes' & 'scores'\n",
    "    boxes = detections[0]['boxes']  # shape: [N, 4]\n",
    "    scores = detections[0]['scores']  # shape: [N]\n",
    "    \n",
    "    # Find the largest face above a certain threshold\n",
    "    best_box = largest_box(boxes, scores, score_threshold=0.5)\n",
    "    \n",
    "    # Create a mask for background blur\n",
    "    mask = np.zeros_like(processed_frame, dtype=np.uint8)\n",
    "    \n",
    "    if best_box is not None:\n",
    "        x1, y1, x2, y2 = map(int, best_box)\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(x2, processed_frame.shape[1]-1), min(y2, processed_frame.shape[0]-1)\n",
    "        \n",
    "        cv2.rectangle(mask, (x1, y1), (x2, y2), (255,255,255), -1)\n",
    "    \n",
    "    # Blur background\n",
    "    blurred_frame = cv2.GaussianBlur(processed_frame, (21,21), 0)\n",
    "    combined_frame = np.where(mask == np.array([255,255,255]), processed_frame, blurred_frame)\n",
    "\n",
    "    current_time = time.time()\n",
    "    if current_time - start_time >= update_interval:\n",
    "        if best_box is not None:\n",
    "            try:\n",
    "                face_roi = combined_frame[y1:y2, x1:x2].copy()\n",
    "                # Convert to grayscale (if your emotion model expects grayscale)\n",
    "                face_gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Possibly do additional preprocessing (e.g., resize to 48x48)\n",
    "                face_resized = cv2.resize(face_gray, (48, 48))  # adapt to your model\n",
    "                face_resized = face_resized.astype(np.float32) / 255.0\n",
    "                # shape: [48,48]\n",
    "                face_resized = np.expand_dims(face_resized, axis=0)   # [1,48,48]\n",
    "                face_resized = np.expand_dims(face_resized, axis=0)   # [1,1,48,48]\n",
    "                \n",
    "                face_tensor = torch.from_numpy(face_resized).to(device)\n",
    "                \n",
    "                # Forward pass through your emotion model\n",
    "                with torch.no_grad():\n",
    "                    logits = emotion_model(face_tensor)\n",
    "                    probs = F.softmax(logits, dim=1)  # shape: [1,7]\n",
    "                probs_np = probs.cpu().numpy()[0]\n",
    "                \n",
    "                emotion_idx = np.argmax(probs_np)\n",
    "                emotion_str = emotion_labels[emotion_idx]\n",
    "                \n",
    "                # Increment count for this emotion\n",
    "                emotion_counts[emotion_str] += 1\n",
    "                \n",
    "                # Draw bounding box and label\n",
    "                color = (255, 0, 0)  # blue\n",
    "                cv2.rectangle(combined_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(combined_frame, emotion_str, (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "                \n",
    "                # Update bar chart\n",
    "                for bar, label in zip(bars, emotion_labels):\n",
    "                    bar.set_height(emotion_counts[label])\n",
    "                max_count = max(emotion_counts.values()) if emotion_counts else 10\n",
    "                ax.set_ylim(0, max(max_count + 5, 10))\n",
    "                fig.canvas.draw()\n",
    "                fig.canvas.flush_events()\n",
    "                plt.pause(0.001)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(\"Error processing face:\", e)\n",
    "        \n",
    "        start_time = current_time\n",
    "    \n",
    "    # Display frames\n",
    "    cv2.imshow(\"Original\", frame)\n",
    "    cv2.imshow(\"Processed + Blur\", combined_frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
